{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Google's pretrained word2vec model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Software/anaconda3/envs/dl/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "imports\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import gensim                             # belongs to gensim package\n",
    "from smart_open import open as smart_open # belongs to smart_open package\n",
    "\n",
    "\"\"\"\n",
    "load model, located in a 'models' folder\n",
    "download it yourself, and do not put it in the github repo\n",
    "(i.e. put it in your .gitignore file) because it's YUGE\n",
    "download: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing\n",
    "\"\"\"\n",
    "with smart_open('./models/GoogleNews-vectors-negative300.bin', 'rb') as word2vec_file:\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_file, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Word2VecKeyedVectors in module gensim.models.keyedvectors object:\n",
      "\n",
      "class Word2VecKeyedVectors(WordEmbeddingsKeyedVectors)\n",
      " |  Word2VecKeyedVectors(vector_size)\n",
      " |  \n",
      " |  Mapping between words and vectors for the :class:`~gensim.models.Word2Vec` model.\n",
      " |  Used to perform operations on the vectors such as vector lookup, distance, similarity etc.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Word2VecKeyedVectors\n",
      " |      WordEmbeddingsKeyedVectors\n",
      " |      BaseKeyedVectors\n",
      " |      gensim.utils.SaveLoad\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  get_keras_embedding(self, train_embeddings=False)\n",
      " |      Get a Keras 'Embedding' layer with weights set as the Word2Vec model's learned word embeddings.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      train_embeddings : bool\n",
      " |          If False, the weights are frozen and stopped from being updated.\n",
      " |          If True, the weights can/will be further trained/updated.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      `keras.layers.Embedding`\n",
      " |          Embedding layer.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      ImportError\n",
      " |          If `Keras <https://pypi.org/project/Keras/>`_ not installed.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      Current method work only if `Keras <https://pypi.org/project/Keras/>`_ installed.\n",
      " |  \n",
      " |  save_word2vec_format(self, fname, fvocab=None, binary=False, total_vec=None)\n",
      " |      Store the input-hidden weight matrix in the same format used by the original\n",
      " |      C word2vec-tool, for compatibility.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          The file path used to save the vectors in\n",
      " |      fvocab : str, optional\n",
      " |          Optional file path used to save the vocabulary\n",
      " |      binary : bool, optional\n",
      " |          If True, the data will be saved in binary word2vec format, else it will be saved in plain text.\n",
      " |      total_vec : int, optional\n",
      " |          Optional parameter to explicitly specify total no. of vectors\n",
      " |          (in case word vectors are appended with document vectors afterwards).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load(fname_or_handle, **kwargs) from builtins.type\n",
      " |      Load an object previously saved using :meth:`~gensim.utils.SaveLoad.save` from a file.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to file that contains needed object.\n",
      " |      mmap : str, optional\n",
      " |          Memory-map option.  If the object was saved with large arrays stored separately, you can load these arrays\n",
      " |          via mmap (shared memory) using `mmap='r'.\n",
      " |          If the file being loaded is compressed (either '.gz' or '.bz2'), then `mmap=None` **must be** set.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.utils.SaveLoad.save`\n",
      " |          Save object to file.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      object\n",
      " |          Object loaded from `fname`.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      AttributeError\n",
      " |          When called on an object instance instead of class (this is a class method).\n",
      " |  \n",
      " |  load_word2vec_format(fname, fvocab=None, binary=False, encoding='utf8', unicode_errors='strict', limit=None, datatype=<class 'numpy.float32'>) from builtins.type\n",
      " |      Load the input-hidden weight matrix from the original C word2vec-tool format.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      The information stored in the file is incomplete (the binary tree is missing),\n",
      " |      so while you can query for word similarity etc., you cannot continue training\n",
      " |      with a model loaded this way.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          The file path to the saved word2vec-format file.\n",
      " |      fvocab : str, optional\n",
      " |          File path to the vocabulary.Word counts are read from `fvocab` filename, if set\n",
      " |          (this is the file generated by `-save-vocab` flag of the original C tool).\n",
      " |      binary : bool, optional\n",
      " |          If True, indicates whether the data is in binary word2vec format.\n",
      " |      encoding : str, optional\n",
      " |          If you trained the C model using non-utf8 encoding for words, specify that encoding in `encoding`.\n",
      " |      unicode_errors : str, optional\n",
      " |          default 'strict', is a string suitable to be passed as the `errors`\n",
      " |          argument to the unicode() (Python 2.x) or str() (Python 3.x) function. If your source\n",
      " |          file may include word tokens truncated in the middle of a multibyte unicode character\n",
      " |          (as is common from the original word2vec.c tool), 'ignore' or 'replace' may help.\n",
      " |      limit : int, optional\n",
      " |          Sets a maximum number of word-vectors to read from the file. The default,\n",
      " |          None, means read all.\n",
      " |      datatype : type, optional\n",
      " |          (Experimental) Can coerce dimensions to a non-default float type (such as `np.float16`) to save memory.\n",
      " |          Such types may result in much slower bulk operations or incompatibility with optimized routines.)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`~gensim.models.keyedvectors.Word2VecKeyedVectors`\n",
      " |          Loaded model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from WordEmbeddingsKeyedVectors:\n",
      " |  \n",
      " |  __contains__(self, word)\n",
      " |  \n",
      " |  __init__(self, vector_size)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  accuracy(self, questions, restrict_vocab=30000, most_similar=<function WordEmbeddingsKeyedVectors.most_similar at 0x7f8bd13a4d90>, case_insensitive=True)\n",
      " |      Compute accuracy of the model.\n",
      " |      \n",
      " |      The accuracy is reported (=printed to log and returned as a list) for each\n",
      " |      section separately, plus there's one aggregate summary at the end.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      questions : str\n",
      " |          Path to file, where lines are 4-tuples of words, split into sections by \": SECTION NAME\" lines.\n",
      " |          See `gensim/test/test_data/questions-words.txt` as example.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.\n",
      " |          This may be meaningful if you've sorted the model vocabulary by descending frequency (which is standard\n",
      " |          in modern word embedding models).\n",
      " |      most_similar : function, optional\n",
      " |          Function used for similarity calculation.\n",
      " |      case_insensitive : bool, optional\n",
      " |          If True - convert all words to their uppercase form before evaluating the performance.\n",
      " |          Useful to handle case-mismatch between training tokens and words in the test set.\n",
      " |          In case of multiple case variants of a single word, the vector for the first occurrence\n",
      " |          (also the most frequent if vocabulary is sorted) is taken.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of dict of (str, (str, str, str)\n",
      " |          Full lists of correct and incorrect predictions divided by sections.\n",
      " |  \n",
      " |  distance(self, w1, w2)\n",
      " |      Compute cosine distance between two words.\n",
      " |      Calculate 1 - :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similarity`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      w1 : str\n",
      " |          Input word.\n",
      " |      w2 : str\n",
      " |          Input word.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Distance between `w1` and `w2`.\n",
      " |  \n",
      " |  distances(self, word_or_vector, other_words=())\n",
      " |      Compute cosine distances from given word or vector to all words in `other_words`.\n",
      " |      If `other_words` is empty, return distance between `word_or_vectors` and all words in vocab.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word_or_vector : {str, numpy.ndarray}\n",
      " |          Word or vector from which distances are to be computed.\n",
      " |      other_words : iterable of str\n",
      " |          For each word in `other_words` distance from `word_or_vector` is computed.\n",
      " |          If None or empty, distance of `word_or_vector` from all words in vocab is computed (including itself).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.array\n",
      " |          Array containing distances to all words in `other_words` from input `word_or_vector`.\n",
      " |      \n",
      " |      Raises\n",
      " |      -----\n",
      " |      KeyError\n",
      " |          If either `word_or_vector` or any word in `other_words` is absent from vocab.\n",
      " |  \n",
      " |  doesnt_match(self, words)\n",
      " |      Which word from the given list doesn't go with the others?\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      words : list of str\n",
      " |          List of words.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          The word further away from the mean of all words.\n",
      " |  \n",
      " |  evaluate_word_analogies(self, analogies, restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n",
      " |      Compute performance of the model on an analogy test set.\n",
      " |      \n",
      " |      This is modern variant of :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.accuracy`, see\n",
      " |      `discussion on GitHub #1935 <https://github.com/RaRe-Technologies/gensim/pull/1935>`_.\n",
      " |      \n",
      " |      The accuracy is reported (printed to log and returned as a score) for each section separately,\n",
      " |      plus there's one aggregate summary at the end.\n",
      " |      \n",
      " |      This method corresponds to the `compute-accuracy` script of the original C word2vec.\n",
      " |      See also `Analogy (State of the art) <https://aclweb.org/aclwiki/Analogy_(State_of_the_art)>`_.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      analogies : str\n",
      " |          Path to file, where lines are 4-tuples of words, split into sections by \": SECTION NAME\" lines.\n",
      " |          See `gensim/test/test_data/questions-words.txt` as example.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.\n",
      " |          This may be meaningful if you've sorted the model vocabulary by descending frequency (which is standard\n",
      " |          in modern word embedding models).\n",
      " |      case_insensitive : bool, optional\n",
      " |          If True - convert all words to their uppercase form before evaluating the performance.\n",
      " |          Useful to handle case-mismatch between training tokens and words in the test set.\n",
      " |          In case of multiple case variants of a single word, the vector for the first occurrence\n",
      " |          (also the most frequent if vocabulary is sorted) is taken.\n",
      " |      dummy4unknown : bool, optional\n",
      " |          If True - produce zero accuracies for 4-tuples with out-of-vocabulary words.\n",
      " |          Otherwise, these tuples are skipped entirely and not used in the evaluation.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          The overall evaluation score on the entire evaluation set\n",
      " |      sections : list of dict of {str : str or list of tuple of (str, str, str, str)}\n",
      " |          Results broken down by each section of the evaluation set. Each dict contains the name of the section\n",
      " |          under the key 'section', and lists of correctly and incorrectly predicted 4-tuples of words under the\n",
      " |          keys 'correct' and 'incorrect'.\n",
      " |  \n",
      " |  evaluate_word_pairs(self, pairs, delimiter='\\t', restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n",
      " |      Compute correlation of the model with human similarity judgments.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      More datasets can be found at\n",
      " |      * http://technion.ac.il/~ira.leviant/MultilingualVSMdata.html\n",
      " |      * https://www.cl.cam.ac.uk/~fh295/simlex.html.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      pairs : str\n",
      " |          Path to file, where lines are 3-tuples, each consisting of a word pair and a similarity value.\n",
      " |          See `test/test_data/wordsim353.tsv` as example.\n",
      " |      delimiter : str, optional\n",
      " |          Separator in `pairs` file.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.\n",
      " |          This may be meaningful if you've sorted the model vocabulary by descending frequency (which is standard\n",
      " |          in modern word embedding models).\n",
      " |      case_insensitive : bool, optional\n",
      " |          If True - convert all words to their uppercase form before evaluating the performance.\n",
      " |          Useful to handle case-mismatch between training tokens and words in the test set.\n",
      " |          In case of multiple case variants of a single word, the vector for the first occurrence\n",
      " |          (also the most frequent if vocabulary is sorted) is taken.\n",
      " |      dummy4unknown : bool, optional\n",
      " |          If True - produce zero accuracies for 4-tuples with out-of-vocabulary words.\n",
      " |          Otherwise, these tuples are skipped entirely and not used in the evaluation.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      pearson : tuple of (float, float)\n",
      " |          Pearson correlation coefficient with 2-tailed p-value.\n",
      " |      spearman : tuple of (float, float)\n",
      " |          Spearman rank-order correlation coefficient between the similarities from the dataset and the\n",
      " |          similarities produced by the model itself, with 2-tailed p-value.\n",
      " |      oov_ratio : float\n",
      " |          The ratio of pairs with unknown words.\n",
      " |  \n",
      " |  get_vector(self, word)\n",
      " |      Get the entity's representations in vector space, as a 1D numpy array.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      entity : str\n",
      " |          Identifier of the entity to return the vector for.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Vector for the specified entity.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      KeyError\n",
      " |          If the given entity identifier doesn't exist.\n",
      " |  \n",
      " |  init_sims(self, replace=False)\n",
      " |      Precompute L2-normalized vectors.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      replace : bool, optional\n",
      " |          If True - forget the original vectors and only keep the normalized ones = saves lots of memory!\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      You **cannot continue training** after doing a replace.\n",
      " |      The model becomes effectively read-only: you can call\n",
      " |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar`,\n",
      " |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similarity`, etc., but not train.\n",
      " |  \n",
      " |  most_similar(self, positive=None, negative=None, topn=10, restrict_vocab=None, indexer=None)\n",
      " |      Find the top-N most similar words.\n",
      " |      Positive words contribute positively towards the similarity, negative words negatively.\n",
      " |      \n",
      " |      This method computes cosine similarity between a simple mean of the projection\n",
      " |      weight vectors of the given words and the vectors for each word in the model.\n",
      " |      The method corresponds to the `word-analogy` and `distance` scripts in the original\n",
      " |      word2vec implementation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      positive : list of str, optional\n",
      " |          List of words that contribute positively.\n",
      " |      negative : list of str, optional\n",
      " |          List of words that contribute negatively.\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar words to return, when `topn` is int. When `topn` is None,\n",
      " |          then similarities for all words are returned.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Optional integer which limits the range of vectors which\n",
      " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |          only check the first 10000 word vectors in the vocabulary order. (This may be\n",
      " |          meaningful if you've sorted the vocabulary by descending frequency.)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (word, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all words are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  most_similar_cosmul(self, positive=None, negative=None, topn=10)\n",
      " |      Find the top-N most similar words, using the multiplicative combination objective,\n",
      " |      proposed by `Omer Levy and Yoav Goldberg \"Linguistic Regularities in Sparse and Explicit Word Representations\"\n",
      " |      <http://www.aclweb.org/anthology/W14-1618>`_. Positive words still contribute positively towards the similarity,\n",
      " |      negative words negatively, but with less susceptibility to one large distance dominating the calculation.\n",
      " |      In the common analogy-solving case, of two positive and one negative examples,\n",
      " |      this method is equivalent to the \"3CosMul\" objective (equation (4)) of Levy and Goldberg.\n",
      " |      \n",
      " |      Additional positive or negative examples contribute to the numerator or denominator,\n",
      " |      respectively - a potentially sensible but untested extension of the method.\n",
      " |      With a single positive example, rankings will be the same as in the default\n",
      " |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      positive : list of str, optional\n",
      " |          List of words that contribute positively.\n",
      " |      negative : list of str, optional\n",
      " |          List of words that contribute negatively.\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar words to return, when `topn` is int. When `topn` is None,\n",
      " |          then similarities for all words are returned.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (word, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all words are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  n_similarity(self, ws1, ws2)\n",
      " |      Compute cosine similarity between two sets of words.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      ws1 : list of str\n",
      " |          Sequence of words.\n",
      " |      ws2: list of str\n",
      " |          Sequence of words.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Similarities between `ws1` and `ws2`.\n",
      " |  \n",
      " |  relative_cosine_similarity(self, wa, wb, topn=10)\n",
      " |      Compute the relative cosine similarity between two words given top-n similar words,\n",
      " |      by `Artuur Leeuwenberga, Mihaela Velab , Jon Dehdaribc, Josef van Genabithbc \"A Minimally Supervised Approach\n",
      " |      for Synonym Extraction with Word Embeddings\" <https://ufal.mff.cuni.cz/pbml/105/art-leeuwenberg-et-al.pdf>`_.\n",
      " |      \n",
      " |      To calculate relative cosine similarity between two words, equation (1) of the paper is used.\n",
      " |      For WordNet synonyms, if rcs(topn=10) is greater than 0.10 then wa and wb are more similar than\n",
      " |      any arbitrary word pairs.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      wa: str\n",
      " |          Word for which we have to look top-n similar word.\n",
      " |      wb: str\n",
      " |          Word for which we evaluating relative cosine similarity with wa.\n",
      " |      topn: int, optional\n",
      " |          Number of top-n similar words to look with respect to wa.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.float64\n",
      " |          Relative cosine similarity between wa and wb.\n",
      " |  \n",
      " |  save(self, *args, **kwargs)\n",
      " |      Save KeyedVectors.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the output file.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.load`\n",
      " |          Load saved model.\n",
      " |  \n",
      " |  similar_by_vector(self, vector, topn=10, restrict_vocab=None)\n",
      " |      Find the top-N most similar words by vector.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vector : numpy.array\n",
      " |          Vector from which similarities are to be computed.\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar words to return, when `topn` is int. When `topn` is None,\n",
      " |          then similarities for all words are returned.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Optional integer which limits the range of vectors which\n",
      " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |          only check the first 10000 word vectors in the vocabulary order. (This may be\n",
      " |          meaningful if you've sorted the vocabulary by descending frequency.)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (word, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all words are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  similar_by_word(self, word, topn=10, restrict_vocab=None)\n",
      " |      Find the top-N most similar words.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word : str\n",
      " |          Word\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar words to return. If topn is None, similar_by_word returns\n",
      " |          the vector of similarity scores.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Optional integer which limits the range of vectors which\n",
      " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |          only check the first 10000 word vectors in the vocabulary order. (This may be\n",
      " |          meaningful if you've sorted the vocabulary by descending frequency.)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (word, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all words are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  similarity(self, w1, w2)\n",
      " |      Compute cosine similarity between two words.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      w1 : str\n",
      " |          Input word.\n",
      " |      w2 : str\n",
      " |          Input word.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Cosine similarity between `w1` and `w2`.\n",
      " |  \n",
      " |  similarity_matrix(self, dictionary, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100, dtype=<class 'numpy.float32'>)\n",
      " |      Construct a term similarity matrix for computing Soft Cosine Measure.\n",
      " |      \n",
      " |      This creates a sparse term similarity matrix in the :class:`scipy.sparse.csc_matrix` format for computing\n",
      " |      Soft Cosine Measure between documents.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dictionary : :class:`~gensim.corpora.dictionary.Dictionary`\n",
      " |          A dictionary that specifies the considered terms.\n",
      " |      tfidf : :class:`gensim.models.tfidfmodel.TfidfModel` or None, optional\n",
      " |          A model that specifies the relative importance of the terms in the dictionary. The\n",
      " |          columns of the term similarity matrix will be build in a decreasing order of importance\n",
      " |          of terms, or in the order of term identifiers if None.\n",
      " |      threshold : float, optional\n",
      " |          Only embeddings more similar than `threshold` are considered when retrieving word\n",
      " |          embeddings closest to a given word embedding.\n",
      " |      exponent : float, optional\n",
      " |          Take the word embedding similarities larger than `threshold` to the power of `exponent`.\n",
      " |      nonzero_limit : int, optional\n",
      " |          The maximum number of non-zero elements outside the diagonal in a single column of the\n",
      " |          sparse term similarity matrix.\n",
      " |      dtype : numpy.dtype, optional\n",
      " |          Data-type of the sparse term similarity matrix.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`scipy.sparse.csc_matrix`\n",
      " |          Term similarity matrix.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :func:`gensim.matutils.softcossim`\n",
      " |          The Soft Cosine Measure.\n",
      " |      :class:`~gensim.similarities.docsim.SoftCosineSimilarity`\n",
      " |          A class for performing corpus-based similarity queries with Soft Cosine Measure.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The constructed matrix corresponds to the matrix Mrel defined in section 2.1 of\n",
      " |      `Delphine Charlet and Geraldine Damnati, \"SimBow at SemEval-2017 Task 3: Soft-Cosine Semantic Similarity\n",
      " |      between Questions for Community Question Answering\", 2017\n",
      " |      <http://www.aclweb.org/anthology/S/S17/S17-2051.pdf>`_.\n",
      " |  \n",
      " |  wmdistance(self, document1, document2)\n",
      " |      Compute the Word Mover's Distance between two documents.\n",
      " |      \n",
      " |      When using this code, please consider citing the following papers:\n",
      " |      \n",
      " |      * `Ofir Pele and Michael Werman \"A linear time histogram metric for improved SIFT matching\"\n",
      " |        <http://www.cs.huji.ac.il/~werman/Papers/ECCV2008.pdf>`_\n",
      " |      * `Ofir Pele and Michael Werman \"Fast and robust earth mover's distances\"\n",
      " |        <https://ieeexplore.ieee.org/document/5459199/>`_\n",
      " |      * `Matt Kusner et al. \"From Word Embeddings To Document Distances\"\n",
      " |        <http://proceedings.mlr.press/v37/kusnerb15.pdf>`_.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      document1 : list of str\n",
      " |          Input document.\n",
      " |      document2 : list of str\n",
      " |          Input document.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Word Mover's distance between `document1` and `document2`.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      This method only works if `pyemd <https://pypi.org/project/pyemd/>`_ is installed.\n",
      " |      \n",
      " |      If one of the documents have no words that exist in the vocab, `float('inf')` (i.e. infinity)\n",
      " |      will be returned.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      ImportError\n",
      " |          If `pyemd <https://pypi.org/project/pyemd/>`_  isn't installed.\n",
      " |  \n",
      " |  word_vec(self, word, use_norm=False)\n",
      " |      Get `word` representations in vector space, as a 1D numpy array.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word : str\n",
      " |          Input word\n",
      " |      use_norm : bool, optional\n",
      " |          If True - resulting vector will be L2-normalized (unit euclidean length).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Vector representation of `word`.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      KeyError\n",
      " |          If word not in vocabulary.\n",
      " |  \n",
      " |  words_closer_than(self, w1, w2)\n",
      " |      Get all words that are closer to `w1` than `w2` is to `w1`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      w1 : str\n",
      " |          Input word.\n",
      " |      w2 : str\n",
      " |          Input word.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list (str)\n",
      " |          List of words that are closer to `w1` than `w2` is to `w1`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from WordEmbeddingsKeyedVectors:\n",
      " |  \n",
      " |  cosine_similarities(vector_1, vectors_all)\n",
      " |      Compute cosine similarities between one vector and a set of other vectors.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vector_1 : numpy.ndarray\n",
      " |          Vector from which similarities are to be computed, expected shape (dim,).\n",
      " |      vectors_all : numpy.ndarray\n",
      " |          For each row in vectors_all, distance from vector_1 is computed, expected shape (num_vectors, dim).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Contains cosine distance between `vector_1` and each row in `vectors_all`, shape (num_vectors,).\n",
      " |  \n",
      " |  log_accuracy(section)\n",
      " |  \n",
      " |  log_evaluate_word_pairs(pearson, spearman, oov, pairs)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from WordEmbeddingsKeyedVectors:\n",
      " |  \n",
      " |  index2entity\n",
      " |  \n",
      " |  syn0\n",
      " |  \n",
      " |  syn0norm\n",
      " |  \n",
      " |  wv\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseKeyedVectors:\n",
      " |  \n",
      " |  __getitem__(self, entities)\n",
      " |      Get vector representation of `entities`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      entities : {str, list of str}\n",
      " |          Input entity/entities.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Vector representation for `entities` (1D if `entities` is string, otherwise - 2D).\n",
      " |  \n",
      " |  __setitem__(self, entities, weights)\n",
      " |      Add entities and theirs vectors in a manual way.\n",
      " |      If some entity is already in the vocabulary, old vector is replaced with the new one.\n",
      " |      This method is alias for :meth:`~gensim.models.keyedvectors.BaseKeyedVectors.add` with `replace=True`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      entities : {str, list of str}\n",
      " |          Entities specified by their string ids.\n",
      " |      weights: list of numpy.ndarray or numpy.ndarray\n",
      " |          List of 1D np.array vectors or 2D np.array of vectors.\n",
      " |  \n",
      " |  add(self, entities, weights, replace=False)\n",
      " |      Append entities and theirs vectors in a manual way.\n",
      " |      If some entity is already in the vocabulary, the old vector is kept unless `replace` flag is True.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      entities : list of str\n",
      " |          Entities specified by string ids.\n",
      " |      weights: list of numpy.ndarray or numpy.ndarray\n",
      " |          List of 1D np.array vectors or a 2D np.array of vectors.\n",
      " |      replace: bool, optional\n",
      " |          Flag indicating whether to replace vectors for entities which already exist in the vocabulary,\n",
      " |          if True - replace vectors, otherwise - keep old vectors.\n",
      " |  \n",
      " |  closer_than(self, entity1, entity2)\n",
      " |      Get all entities that are closer to `entity1` than `entity2` is to `entity1`.\n",
      " |  \n",
      " |  most_similar_to_given(self, entity1, entities_list)\n",
      " |      Get the `entity` from `entities_list` most similar to `entity1`.\n",
      " |  \n",
      " |  rank(self, entity1, entity2)\n",
      " |      Rank of the distance of `entity2` from `entity1`, in relation to distances of all entities from `entity1`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Putting model to use\n",
    "notes:\n",
    " - model is made to produce 300D vectors\n",
    " - can't turn vectors into words\n",
    "\"\"\"\n",
    "model.get_vector(\"hi\")\n",
    "model.get_vector(\"Yagmur\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the arxiv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "DATAPATH = \"data/articles.json\"\n",
    "\n",
    "with open(DATAPATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    articles = json.load(f)\n",
    "    \n",
    "all_categories = []\n",
    "articles_dict = dict()\n",
    "for article in articles:\n",
    "    category = article[\"category\"]\n",
    "    if category not in all_categories:\n",
    "        all_categories.append(category)\n",
    "        articles_dict[category] = []\n",
    "    articles_dict[category].append(article)\n",
    "    \n",
    "n_categories = len(arcticles_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning abstracts into pytorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " 'David',\n",
       " \"I'm\",\n",
       " 'just',\n",
       " 'testing',\n",
       " 'this',\n",
       " 'Please',\n",
       " 'pay',\n",
       " 'attention',\n",
       " 'to',\n",
       " 'me']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "text = \"Hi David, I'm just testing this. Please pay attention to me.\"\n",
    "y = lambda x: re.sub(\"[^\\w'-]\", \" \",  x).split()\n",
    "y(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 300])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import string\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "tokenize = lambda x: re.sub(\"[^\\w'-]\", \" \",  x).split()\n",
    "\n",
    "\n",
    "def lineToTensor(text):\n",
    "    words = tokenize(text)\n",
    "    vectors = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            vector = model.get_vector(word)\n",
    "            vectors.append(vector)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    #create 3D tensor with the shape which is proper for the LSTM\n",
    "    return torch.tensor([[v] for v in vectors])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the RNN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.i2h = nn.LSTM(input_size, hidden_size, 1)\n",
    "        \n",
    "        \n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        hidden , (h_t, c_t) = self.i2h(input)\n",
    "        output = self.h2o(hidden[-1])\n",
    "        output = self.softmax(output)\n",
    "        return output\n",
    "    \n",
    "rnn=LSTM(300, 300, n_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Economics', 2)\n"
     ]
    }
   ],
   "source": [
    "def categoryFromOutput(out):\n",
    "    top_n, top_i = out.topk(1)\n",
    "    category_i = top_i[0].item()\n",
    "    return all_categories[category_i], category_i\n",
    "\n",
    "print(categoryFromOutput(rnn(lineToTensor(\"some random line here\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Quantitative Biology',\n",
       " 'We analyze the fate of dynamical systems that consist of two kind of\\nprocesses. The first type is supposed to perform a certain function by\\nprocessing information at a required high accuracy, which is, however, limited\\nto less than 100 percent, while the second process serves to maintain the\\nrequired precision. Both processes are assumed to be subject to a trade-off\\nbetween cost and precision, where the cost have to be paid from renewable but\\nlimited resources. In a discrete map we pursue the time evolution of errors and\\ndetermine the conditions under which the fate of the system is either a stable\\nperformance at the desired accuracy, or a deterioration. Deterioration may be\\nrealized either as an accumulation of errors or a decline of resources when\\nthey are all absorbed for maintenance. We point to possible implications for\\nliving organisms and their perspectives to avoid an accumulation of errors in\\nthe course of time.',\n",
       " tensor([5]),\n",
       " tensor([[[-0.2891,  0.3262,  0.1895,  ...,  0.1377,  0.0938, -0.1592]],\n",
       " \n",
       "         [[-0.2275, -0.1045,  0.1455,  ..., -0.0986, -0.0928, -0.0596]],\n",
       " \n",
       "         [[ 0.0801,  0.1050,  0.0498,  ...,  0.0037,  0.0476, -0.0688]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.0801,  0.1050,  0.0498,  ...,  0.0037,  0.0476, -0.0688]],\n",
       " \n",
       "         [[ 0.0306,  0.0708,  0.0723,  ..., -0.0099, -0.0245,  0.0140]],\n",
       " \n",
       "         [[-0.0474,  0.1875,  0.0023,  ..., -0.0036, -0.0625, -0.0557]]]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import choice\n",
    "\n",
    "def randomTrainingExample():\n",
    "    category = choice(all_categories)\n",
    "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "    article = choice(articles_dict[category])\n",
    "    abstract = article[\"abstract\"]\n",
    "    abstract_tensor = lineToTensor(abstract)\n",
    "    return category, abstract, category_tensor, abstract_tensor\n",
    "\n",
    "randomTrainingExample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(rnn.parameters(), lr = learning_rate, momentum = 0.9)\n",
    "\n",
    "def train(category_tensor, abstract_tensor):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output = rnn(abstract_tensor)\n",
    "            \n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "        \n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 20.0% (0.0m 22.738040447235107s) 1.786 / Computer Science V\n",
      "200 40.0% (0.0m 40.76829504966736s) 2.212 / Economics X Electrical Engineering and Systems Science\n",
      "300 60.0% (0.0m 57.51316237449646s) 1.719 / Physics V\n",
      "400 80.0% (1.0m 13.751345157623291s) 2.341 / Electrical Engineering and Systems Science X Computer Science\n",
      "500 100.0% (1.0m 29.86014485359192s) 2.19 / Electrical Engineering and Systems Science X Quantitative Finance\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "n_iters = 500\n",
    "print_every = 100\n",
    "plot_every = 10\n",
    "\n",
    "# keep track of losses for plotting\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = s//60\n",
    "    s -= m*60\n",
    "    return \"{}m {}s\".format(m, s)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for it in range(1, n_iters+1):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    output, loss = train(category_tensor, line_tensor)\n",
    "    current_loss += loss\n",
    "    \n",
    "    if not (it % print_every):\n",
    "        guess, guess_i = categoryFromOutput(output)\n",
    "        correct = \"V\" if guess == category else \"X {}\".format(category)\n",
    "        print(\"{} {}% ({}) {:.4} / {} {}\".format(it, it/n_iters*100, timeSince(start), loss, guess, correct))\n",
    "        \n",
    "    if not (it % plot_every):\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8a5073b0f0>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
