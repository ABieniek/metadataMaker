{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Google's pretrained word2vec model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Software/anaconda3/envs/dl/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "imports\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import gensim                             # belongs to gensim package\n",
    "from smart_open import open as smart_open # belongs to smart_open package\n",
    "\n",
    "\"\"\"\n",
    "load model, located in a 'models' folder\n",
    "download it yourself, and do not put it in the github repo\n",
    "(i.e. put it in your .gitignore file) because it's YUGE\n",
    "download: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing\n",
    "\"\"\"\n",
    "with smart_open('./models/GoogleNews-vectors-negative300.bin', 'rb') as word2vec_file:\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_file, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0612793 , -0.01312256, -0.02307129, -0.04736328,  0.02270508,\n",
       "        0.12695312, -0.06396484, -0.22949219, -0.01116943,  0.04931641,\n",
       "       -0.01977539, -0.06738281, -0.18457031,  0.02856445, -0.06445312,\n",
       "       -0.07861328, -0.04321289, -0.08300781, -0.10009766, -0.08496094,\n",
       "       -0.01409912,  0.12109375,  0.01855469,  0.03051758,  0.00610352,\n",
       "       -0.11865234,  0.08105469, -0.01220703,  0.09716797, -0.16992188,\n",
       "       -0.1171875 , -0.12011719,  0.10302734,  0.05249023, -0.01904297,\n",
       "       -0.01696777, -0.11669922,  0.16503906, -0.10986328,  0.04785156,\n",
       "        0.13769531, -0.00488281, -0.0534668 , -0.08789062,  0.12890625,\n",
       "       -0.05029297, -0.12890625, -0.13769531, -0.03833008, -0.07226562,\n",
       "       -0.10351562,  0.03930664,  0.07714844,  0.03686523,  0.0402832 ,\n",
       "        0.05834961,  0.03442383,  0.17578125,  0.0324707 , -0.04882812,\n",
       "        0.00497437, -0.078125  ,  0.02050781, -0.01116943,  0.14160156,\n",
       "        0.00282288,  0.22851562, -0.05078125,  0.08642578, -0.02197266,\n",
       "       -0.12890625, -0.02111816, -0.10253906, -0.0378418 , -0.24707031,\n",
       "       -0.02099609, -0.03735352, -0.0378418 ,  0.05322266,  0.03222656,\n",
       "        0.03808594, -0.12060547,  0.01019287, -0.10791016, -0.05981445,\n",
       "        0.03833008, -0.0859375 ,  0.07177734, -0.03149414,  0.03466797,\n",
       "       -0.08447266, -0.17578125, -0.02453613,  0.01940918, -0.03588867,\n",
       "        0.00026321,  0.05834961,  0.08398438,  0.00527954,  0.01000977,\n",
       "        0.03271484, -0.08789062, -0.109375  ,  0.09082031,  0.06835938,\n",
       "        0.12597656, -0.00921631, -0.10595703,  0.02197266,  0.09521484,\n",
       "       -0.05078125, -0.03735352,  0.02026367, -0.1640625 , -0.04956055,\n",
       "       -0.04394531, -0.12597656,  0.06396484, -0.09228516,  0.0234375 ,\n",
       "       -0.04077148,  0.13085938, -0.10546875, -0.15917969, -0.11767578,\n",
       "        0.0279541 , -0.05151367, -0.09863281, -0.08984375, -0.00866699,\n",
       "        0.03173828, -0.05786133,  0.00341797,  0.07568359,  0.02978516,\n",
       "        0.02832031,  0.02819824,  0.05322266,  0.03320312,  0.05664062,\n",
       "        0.04150391, -0.01300049, -0.03442383,  0.0168457 , -0.10742188,\n",
       "        0.12597656, -0.06176758,  0.03466797, -0.07763672, -0.1484375 ,\n",
       "        0.13867188,  0.06054688,  0.01123047, -0.01879883,  0.15527344,\n",
       "        0.03491211, -0.12060547, -0.03759766,  0.01147461, -0.20605469,\n",
       "       -0.05175781, -0.09765625,  0.12695312, -0.10791016,  0.01531982,\n",
       "        0.04760742,  0.14257812,  0.01068115, -0.07470703,  0.03637695,\n",
       "       -0.09667969, -0.00408936, -0.0534668 , -0.08984375, -0.03515625,\n",
       "        0.08349609,  0.01531982,  0.01721191,  0.03662109, -0.13378906,\n",
       "        0.19824219, -0.10009766, -0.0168457 ,  0.07275391, -0.02856445,\n",
       "        0.13085938,  0.03808594, -0.1171875 , -0.078125  ,  0.10839844,\n",
       "        0.00315857, -0.01831055,  0.02990723,  0.19726562,  0.04882812,\n",
       "        0.00323486,  0.06030273,  0.05859375,  0.02209473,  0.05737305,\n",
       "       -0.06396484,  0.05541992, -0.0559082 ,  0.04663086,  0.04443359,\n",
       "       -0.24023438,  0.09375   , -0.0234375 , -0.07910156, -0.0111084 ,\n",
       "       -0.01306152, -0.03686523, -0.07958984, -0.06689453, -0.13085938,\n",
       "        0.05786133, -0.1171875 , -0.04833984, -0.00787354, -0.10888672,\n",
       "       -0.09765625,  0.01855469,  0.07324219, -0.05981445,  0.08007812,\n",
       "       -0.07080078,  0.02539062, -0.01367188, -0.10791016, -0.05151367,\n",
       "        0.11621094, -0.13671875, -0.07617188, -0.00512695,  0.04199219,\n",
       "       -0.00646973, -0.03686523,  0.11767578,  0.03393555,  0.04345703,\n",
       "       -0.00939941, -0.05395508,  0.06933594,  0.06933594,  0.15820312,\n",
       "        0.05004883, -0.09863281,  0.02441406,  0.11132812, -0.1484375 ,\n",
       "        0.12158203, -0.02050781,  0.09667969,  0.01275635,  0.05004883,\n",
       "        0.01586914, -0.05541992,  0.03564453,  0.05371094, -0.04956055,\n",
       "       -0.13183594, -0.01019287,  0.02294922, -0.12988281,  0.140625  ,\n",
       "        0.12255859,  0.07226562,  0.06640625,  0.05371094,  0.17285156,\n",
       "        0.11621094, -0.07617188,  0.00230408,  0.01245117,  0.09277344,\n",
       "        0.14160156, -0.03881836, -0.12011719, -0.02282715,  0.03039551,\n",
       "       -0.04150391,  0.03125   , -0.09082031, -0.04101562,  0.0255127 ,\n",
       "        0.06494141, -0.02294922,  0.10693359,  0.0050354 , -0.05151367,\n",
       "       -0.01782227, -0.08349609, -0.109375  ,  0.0378418 , -0.03100586,\n",
       "        0.0859375 , -0.01757812, -0.02026367,  0.07568359,  0.22070312],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Putting model to use\n",
    "notes:\n",
    " - model is made to produce 300D vectors\n",
    " - can't turn vectors into words\n",
    "\"\"\"\n",
    "model.get_vector(\"hi\")\n",
    "model.get_vector(\"Yagmur\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the arxiv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "DATAPATH = \"data/articles.json\"\n",
    "\n",
    "with open(DATAPATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    articles = json.load(f)\n",
    "    \n",
    "all_categories = []\n",
    "articles_dict = dict()\n",
    "for article in articles:\n",
    "    category = article[\"category\"]\n",
    "    if category not in all_categories:\n",
    "        all_categories.append(category)\n",
    "        articles_dict[category] = []\n",
    "    articles_dict[category].append(article)\n",
    "    \n",
    "n_categories = len(articles_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning abstracts into pytorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "text = \"Hi David, I'm just testing this. Please pay attention to me.\"\n",
    "y = lambda x: re.sub(\"[^\\w'-]\", \" \",  x).split()\n",
    "y(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 1, 300])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import string\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "tokenize = lambda x: re.sub(\"[^\\w'-]\", \" \",  x).split()\n",
    "\n",
    "\n",
    "def lineToTensor(text):\n",
    "    words = tokenize(text)\n",
    "    vectors = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            vector = model.get_vector(word)\n",
    "            vectors.append(vector)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    #create 3D tensor with the shape which is proper for the LSTM\n",
    "    return torch.tensor([[v] for v in vectors])\n",
    "\n",
    "print (lineToTensor(\"wow look at this hot potato\").shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the RNN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.i2h = nn.LSTM(input_size, hidden_size, 1)\n",
    "        \n",
    "        \n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        hidden , (h_t, c_t) = self.i2h(input)\n",
    "        output = self.h2o(hidden[-1])\n",
    "        output = self.softmax(output)\n",
    "        return output\n",
    "    \n",
    "rnn=LSTM(300, 300, n_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categoryFromOutput(out):\n",
    "    top_n, top_i = out.topk(1)\n",
    "    category_i = top_i[0].item()\n",
    "    return all_categories[category_i], category_i\n",
    "\n",
    "print(categoryFromOutput(rnn(lineToTensor(\"some random line here\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "\n",
    "def randomTrainingExample():\n",
    "    category = choice(all_categories)\n",
    "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "    article = choice(articles_dict[category])\n",
    "    abstract = article[\"abstract\"]\n",
    "    abstract_tensor = lineToTensor(abstract)\n",
    "    return category, abstract, category_tensor, abstract_tensor\n",
    "\n",
    "randomTrainingExample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(rnn.parameters(), lr = learning_rate, momentum = 0.9)\n",
    "\n",
    "def train(category_tensor, abstract_tensor):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output = rnn(abstract_tensor)\n",
    "            \n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "        \n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "n_iters = 1000000\n",
    "print_every = 100\n",
    "plot_every = 10\n",
    "\n",
    "# keep track of losses for plotting\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = s//60\n",
    "    s -= m*60\n",
    "    return \"{}m {}s\".format(m, s)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for it in range(1, n_iters+1):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    output, loss = train(category_tensor, line_tensor)\n",
    "    current_loss += loss\n",
    "    \n",
    "    if not (it % print_every):\n",
    "        guess, guess_i = categoryFromOutput(output)\n",
    "        correct = \"V\" if guess == category else \"X {}\".format(category)\n",
    "        print(\"{} {}% ({}) {:.4} / {} {}\".format(it, it/n_iters*100, timeSince(start), loss, guess, correct))\n",
    "        \n",
    "    if not (it % plot_every):\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
